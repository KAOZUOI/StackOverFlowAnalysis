val deadlines =spark.read.format(&quot;delta&quot;).load(&quot;/path/to/file/&quot;)Intitializing Scala interpreter ...Spark Web UI available at http://jupyter-xxxx:4040SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1672036786616)SparkSession available as 'spark':: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml*Error:*org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (jupyter-XXXXXX executor driver): java.lang.ClassCastException: class org.apache.spark.sql.delta.actions.SingleAction cannot be cast to class org.apache.spark.sql.delta.actions.SingleAction (org.apache.spark.sql.delta.actions.SingleAction is in unnamed module of loader org.apache.spark.util.MutableURLClassLoader @5c089b2f; org.apache.spark.sql.delta.actions.SingleAction is in unnamed module of loader scala.reflect.internal.util.ScalaClassLoader$URLClassLoader @89842c3)    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)    at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)    at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)    at org.apache.spark.scheduler.Task.run(Task.scala:131)    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)    at java.base/java.lang.Thread.run(Thread.java:829)Driver stacktrace:  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)  at scala.Option.foreach(Option.scala:407)  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)  at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)  at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)  at org.apache.spark.sql.Dataset.head(Dataset.scala:2729)  at org.apache.spark.sql.Dataset.first(Dataset.scala:2736)  at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$1(Snapshot.scala:151)  at org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53)  at org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32)  at org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27)  at org.apache.spark.sql.delta.Snapshot.withStatusCode(Snapshot.scala:55)  at org.apache.spark.sql.delta.Snapshot.computedState$lzycompute(Snapshot.scala:137)  at org.apache.spark.sql.delta.Snapshot.computedState(Snapshot.scala:136)  at org.apache.spark.sql.delta.Snapshot.metadata(Snapshot.scala:179)  at org.apache.spark.sql.delta.Snapshot.toString(Snapshot.scala:290)  at java.base/java.lang.String.valueOf(String.java:2951)  at java.base/java.lang.StringBuilder.append(StringBuilder.java:172)  at org.apache.spark.sql.delta.Snapshot.$anonfun$new$1(Snapshot.scala:293)  at org.apache.spark.sql.delta.Snapshot.$anonfun$logInfo$1(Snapshot.scala:270)  at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)  at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)  at org.apache.spark.sql.delta.Snapshot.logInfo(Snapshot.scala:270)  at org.apache.spark.sql.delta.Snapshot.<init>(Snapshot.scala:293)  at org.apache.spark.sql.delta.SnapshotManagement.createSnapshot(SnapshotManagement.scala:223)  at org.apache.spark.sql.delta.SnapshotManagement.createSnapshot$(SnapshotManagement.scala:211)  at org.apache.spark.sql.delta.DeltaLog.createSnapshot(DeltaLog.scala:59)  at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:195)  at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:186)  at org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:59)  at org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:49)  at org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:63)  at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:467)  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)  at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:467)  at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)  at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)  at org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:367)  at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:106)  at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:91)  at org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:367)  at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$1(DeltaLog.scala:466)  at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)  at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)  at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)  at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)  at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)  at com.google.common.cache.LocalCache.get(LocalCache.java:4000)  at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)  at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:464)  at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:401)  at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:73)  at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:73)  at org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:139)  at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:177)  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:355)  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)  at org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:305)  at scala.Option.map(Option.scala:230)  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:265)  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:239)  ... 37 elidedCaused by: java.lang.ClassCastException: class org.apache.spark.sql.delta.actions.SingleAction cannot be cast to class org.apache.spark.sql.delta.actions.SingleAction (org.apache.spark.sql.delta.actions.SingleAction is in unnamed module of loader org.apache.spark.util.MutableURLClassLoader @5c089b2f; org.apache.spark.sql.delta.actions.SingleAction is in unnamed module of loader scala.reflect.internal.util.ScalaClassLoader$URLClassLoader @89842c3)  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)  at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)  at org.apache.spark.scheduler.Task.run(Task.scala:131)  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)  ... 1 more 